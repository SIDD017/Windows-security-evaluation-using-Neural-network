import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define a class for the MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.hidden = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.output = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.hidden(x)
        x = self.relu(x)
        x = self.output(x)
        return x

# Set random seed for reproducibility
# torch.manual_seed(0)

# Define the dimensions of the network
input_size = 9  # Dimensionality of input features
hidden_size = 20  # Number of neurons in the hidden layer
output_size = 1  # Dimensionality of the output (in this case, a single scalar)

# Create an instance of the model
model = MLP(input_size, hidden_size, output_size)

# Print out the model architecture
print(model)

# Define dummy input and target tensors
# input_tensor = torch.randn(32, input_size)  # 32 is the batch size
# target_tensor = torch.randn(32, output_size)
input_tensor = torch.tensor([[2, 0, 12, 0, 1, 1, 1, 3, 1], [5, 5, 16, 90, 1, 1, 1, 1, 1], [10, 15, 8, 0, 0, 0, 1, 3, 1], [7, 15, 8, 90, 1, 1, 1, 4, 1], [9, 15, 4, 0, 1, 1, 1, 0, 1], [9, 17, 12, 45, 1, 0, 0, 1, 1], [10, 15, 12, 90, 1, 0, 1, 4, 1], [6, 25, 16, 90, 0, 0, 0, 0, 1], [6, 12, 12, 135, 1, 0, 0, 4, 1], [2, 7, 16, 0, 1, 1, 0, 2, 0], [3, 3, 12, 45, 1, 1, 0, 0, 0], [12, 22, 8, 0, 0, 0, 0, 4, 0], [2, 25, 12, 90, 1, 1, 0, 5, 1], [12, 17, 16, 135, 1, 1, 1, 3, 0], [4, 12, 4, 135, 1, 1, 1, 2, 0], [14, 25, 16, 135, 0, 1, 1, 1, 1], [0, 10, 16, 135, 1, 1, 0, 0, 0], [3, 3, 16, 180, 0, 1, 1, 5, 0], [12, 7, 16, 45, 1, 0, 0, 1, 0], [1, 22, 12, 90, 0, 1, 0, 2, 0], [12, 22, 8, 135, 0, 1, 0, 2, 0], [2, 5, 16, 180, 0, 0, 1, 2, 0], [13, 3, 0, 135, 1, 0, 0, 1, 0], [7, 0, 0, 90, 0, 0, 1, 1, 0], [2, 15, 8, 90, 0, 0, 0, 3, 0], [0, 15, 8, 180, 1, 0, 0, 2, 0], [0, 17, 12, 90, 1, 0, 1, 3, 0], [14, 25, 4, 135, 0, 0, 0, 4, 1], [6, 3, 12, 0, 1, 1, 1, 3, 1], [0, 7, 0, 135, 1, 0, 0, 2, 1], [11, 22, 8, 45, 1, 0, 0, 4, 1], [8, 25, 0, 180, 1, 1, 0, 5, 1], [6, 5, 4, 180, 1, 0, 0, 2, 0], [10, 3, 16, 0, 0, 1, 0, 0, 1], [13, 5, 4, 0, 0, 0, 0, 0, 1], [14, 17, 16, 90, 1, 0, 1, 1, 1], [5, 3, 4, 0, 1, 1, 0, 4, 0], [15, 17, 8, 45, 1, 1, 1, 0, 0], [11, 0, 8, 0, 1, 1, 1, 1, 1], [6, 3, 16, 45, 1, 1, 0, 4, 0], [0, 12, 12, 180, 1, 1, 0, 0, 0], [12, 15, 16, 135, 1, 0, 1, 4, 0], [11, 0, 16, 0, 0, 1, 1, 4, 0], [3, 5, 16, 180, 1, 0, 1, 5, 0], [0, 20, 8, 90, 0, 0, 0, 2, 0], [3, 10, 8, 180, 0, 0, 1, 4, 0], [8, 25, 12, 180, 0, 0, 0, 5, 0], [7, 10, 16, 45, 0, 1, 1, 1, 0], [9, 15, 16, 180, 0, 1, 1, 5, 0], [13, 10, 8, 90, 0, 1, 1, 1, 0]]).float()
target_tensor = torch.tensor([[0.9833333333333332], [0.8133333333333332], [0.5766666666666665], [0.6933333333333334], [0.6266666666666666], [0.5599999999999999], [0.5766666666666665], [0.39666666666666667], [0.5866666666666667], [0.4966666666666667], [0.49000000000000005], [0.18], [0.6133333333333334], [0.37666666666666665], [0.4366666666666667], [0.6133333333333334], [0.35333333333333333], [0.5566666666666666], [0.36333333333333334], [0.2800000000000001], [0.24666666666666667], [0.3966666666666666], [0.35666666666666674], [0.4666666666666667], [0.21000000000000002], [0.26], [0.32666666666666666], [0.4133333333333334], [0.9066666666666666], [0.6466666666666666], [0.5133333333333334], [0.5633333333333334], [0.3133333333333333], [0.7233333333333334], [0.53], [0.5766666666666665], [0.5066666666666666], [0.3433333333333334], [0.95], [0.54], [0.35333333333333333], [0.27666666666666667], [0.6166666666666666], [0.43], [0.19666666666666666], [0.33666666666666667], [0.16333333333333333], [0.4533333333333333], [0.36000000000000004], [0.4033333333333333]]).float()

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10000
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(input_tensor)
    loss = criterion(outputs, target_tensor)
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print progress
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# After training, you can use the model for predictions
# For example:
test_input = torch.tensor([[1, 3, 6, 0, 0, 0, 0, 0, 0]]).float()  # Example input tensor
print(test_input)
with torch.no_grad():
    model.eval()
    predicted_output = model(test_input)
    print("Predicted output:", predicted_output.item())





# input_size = 9
# hidden_size = 20
# output_size = 1

# # input_data = np.array([[0, 0, 8, 30, 1, 1, 1, 2, 1], [1, 3, 6, 0, 0, 0, 0, 0, 0], [0, 0, 8, 0, 1, 0, 1, 1, 0]])
# # target = np.array([[1.0], [0.32333333333333336], [0.6]])

# input_data = np.array([[15, 22.5, 8, 180, 0, 1, 1, 5, 0], [11, 15, 16, 180, 0, 1, 0, 2, 1], [11, 12.5, 12, 180, 0, 0, 1, 3, 1], [0, 0, 0, 180, 1, 0, 0, 0, 0], [4, 0, 12, 180, 0, 0, 0, 3, 0], [9, 15, 4, 180, 1, 1, 0, 0, 1], [12, 10, 4, 90, 0, 0, 0, 4, 1], [15, 2.5, 16, 45, 1, 1, 1, 5, 1], [13, 25, 0, 180, 1, 0, 1, 3, 0], [0, 0, 4, 45, 0, 1, 1, 5, 0], [10, 20, 4, 180, 0, 0, 0, 0, 0], [5, 17.5, 0, 90, 1, 1, 1, 5, 0], [3, 25, 16, 0, 0, 0, 0, 0, 0], [8, 7.5, 0, 90, 1, 1, 0, 3, 1], [14, 17.5, 8, 45, 1, 0, 1, 0, 1], [0, 22.5, 16, 45, 0, 1, 0, 0, 1], [5, 17.5, 8, 90, 1, 1, 1, 2, 0], [2, 15, 12, 0, 1, 1, 0, 5, 0], [0, 2.5, 4, 0, 1, 1, 1, 2, 1], [12, 25, 4, 180, 0, 1, 1, 3, 1], [11, 17.5, 12, 0, 0, 0, 1, 4, 0], [10, 5, 0, 90, 1, 0, 0, 5, 0], [3, 20, 16, 180, 1, 0, 1, 0, 0], [10, 2.5, 0, 45, 0, 1, 0, 2, 0], [4, 17.5, 12, 180, 1, 0, 0, 3, 1], [3, 17.5, 8, 180, 0, 0, 0, 0, 0], [10, 2.5, 0, 180, 1, 1, 0, 4, 1], [12, 20, 0, 135, 0, 0, 1, 5, 1], [15, 15, 8, 135, 1, 1, 1, 1, 0], [7, 0, 4, 0, 1, 1, 1, 4, 0], [13, 20, 16, 0, 1, 1, 0, 2, 0], [15, 25, 4, 45, 0, 0, 1, 1, 1], [2, 5, 8, 45, 0, 0, 0, 3, 0], [5, 25, 12, 90, 0, 1, 0, 1, 1], [14, 25, 4, 135, 0, 0, 1, 4, 0], [0, 12.5, 4, 0, 0, 1, 0, 2, 1], [3, 2.5, 8, 0, 1, 0, 1, 4, 1], [12, 5, 16, 0, 1, 0, 0, 0, 0], [12, 0, 16, 90, 1, 1, 0, 0, 1], [7, 17.5, 12, 90, 0, 1, 1, 4, 0], [8, 25, 16, 0, 0, 1, 0, 4, 1], [15, 12.5, 8, 135, 0, 0, 0, 2, 0], [12, 12.5, 8, 0, 0, 1, 0, 3, 1], [3, 17.5, 4, 135, 0, 1, 0, 0, 0], [1, 5, 4, 90, 1, 1, 0, 5, 0], [1, 20, 0, 180, 1, 1, 1, 2, 0], [3, 20, 16, 45, 1, 0, 0, 5, 0], [3, 25, 12, 90, 1, 1, 0, 2, 0], [0, 7.5, 16, 180, 0, 1, 1, 2, 0], [13, 22.5, 4, 180, 0, 0, 1, 5, 0]])
# target = np.array([[0.31333333333333335], [0.5766666666666668], [0.6033333333333333], [0.4], [0.45], [0.5266666666666666], [0.5033333333333333], [0.89], [0.21333333333333332], [0.6333333333333334], [0.04666666666666667], [0.36000000000000004], [0.14666666666666667], [0.7133333333333334], [0.5433333333333333], [0.5633333333333332], [0.39333333333333337], [0.3766666666666667], [0.9066666666666666], [0.58], [0.27666666666666667], [0.2966666666666667], [0.21333333333333337], [0.4566666666666667], [0.5433333333333333], [0.14333333333333334], [0.7566666666666667], [0.48], [0.37666666666666665], [0.6333333333333334], [0.31333333333333335], [0.5133333333333333], [0.36333333333333334], [0.5633333333333334], [0.18], [0.6866666666666666], [0.8233333333333335], [0.2966666666666667], [0.7833333333333334], [0.36000000000000004], [0.5966666666666666], [0.23666666666666672], [0.67], [0.21000000000000002], [0.43000000000000005], [0.3466666666666667], [0.24666666666666667], [0.31333333333333335], [0.5133333333333333], [0.18]])

# # instatiate the MLP
# model = MLP(input_size, hidden_size, output_size)
# model.train(input_data, target)

# print("Trained")

# test_in = np.array([[0, 43, 0, 42, 0, 0, 0, 6, 0]])
# test_out = np.array([[0.19666666666666666]])
# output = model.forward(test_in)

# print(output)